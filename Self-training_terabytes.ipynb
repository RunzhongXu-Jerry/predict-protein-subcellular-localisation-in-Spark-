{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import when\n",
    "import time\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col, expr, when\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Self-training\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the parameter in the dataset\n",
    "def analyse_the_data(DataFrame,Label):\n",
    "    # labeled: data with label 1 or 0\n",
    "    # unlabel: data without label(we use -1 to recognize the unlabeled data)\n",
    "    unlabeled = DataFrame.filter(col(Label).isin([-1]))\n",
    "    labeled = DataFrame.filter(col(Label).isin([0,1]))\n",
    "    label_0 = labeled.filter(col(Label).isin([0])).count()\n",
    "    label_1 = labeled.filter(col(Label).isin([1])).count()\n",
    "    ratioNP_P = label_0/label_1\n",
    "    return labeled,unlabeled,ratioNP_P,label_0,label_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vector\n",
    "def vector_assembler(DataFrame):                                  \n",
    "    vecAss = VectorAssembler(\n",
    "        inputCols=[\"I\", \"V\", \"L\", \"F\", \"C\", \"M\", \"A\", \"G\", \"T\", \"S\", \"W\", \"Y\", \"P\", \"H\", \"E\", \"Q\", \"D\", \"N\", \"K\", \"R\",\n",
    "                   \"hdHydro_mean\", \"helical_mean\"], outputCol=\"features\")\n",
    "    DataFrame = vecAss.transform(DataFrame)\n",
    "    return DataFrame.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read and preprocess the training dataset\n",
    "df = spark.read.format('csv').option(\"header\", 'true').load(\"trainselftraining.csv\")\n",
    "df.cache()\n",
    "numerical_cols = df.columns\n",
    "df = df.select([F.col(c).cast(\"float\").alias(c) for c in numerical_cols])\n",
    "df = vector_assembler(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read and preprocess the testing dataset\n",
    "test = spark.read.format('csv').option(\"header\", 'true').load(\"testselftraining.csv\") \n",
    "numerical_colstest = test.columns\n",
    "test = test.select([F.col(c).cast(\"float\").alias(c) for c in numerical_colstest])\n",
    "test = vector_assembler(test)\n",
    "test = test.withColumn(\"Cytoplasm\",when(col(\"Cytoplasm\").between(2,6),1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main body of training and testing\n",
    "def train_the_classifer(DataFrame, MAXITER, Label,Threshold):\n",
    "    # build model\n",
    "    rf = RandomForestClassifier(labelCol=Label, featuresCol=\"features\", maxDepth=5, numTrees=16,seed = 1234567)\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "    \n",
    "    labeled, unlabeled, ratioNP_P, label_0, label_1 = analyse_the_data(DataFrame, Label)\n",
    "    \n",
    "    #training\n",
    "    for i in range(MAXITER):\n",
    "        print(f'Iteration: {i} - labeled size: {labeled.count()},unlabeled size:{unlabeled.count()}')\n",
    "        training = time.time()\n",
    "\n",
    "        pipeline_model = pipeline.fit(labeled)\n",
    "        print(f'Training time: {time.time() - training}')\n",
    "        \n",
    "        # Get confident predictions on the current unlabelled\n",
    "        confidences = time.time()\n",
    "        \n",
    "        #predict\n",
    "        pred_unlabeled = pipeline_model.transform(unlabeled).cache()\n",
    "        pred_unlabeled = pred_unlabeled.withColumn('xs', vector_to_array('probability'))\n",
    "        pred_unlabeled = pred_unlabeled.withColumn('class_0_prob', F.col('xs')[0])\n",
    "        pred_unlabeled = pred_unlabeled.withColumn('class_1_prob', F.col('xs')[1])\n",
    "        \n",
    "        # compute the majority of 1s in each class, and that will help us decide how many we add\n",
    "        zero = pred_unlabeled.filter(pred_unlabeled['class_0_prob'] >= Threshold).cache()\n",
    "        one = pred_unlabeled.filter(pred_unlabeled['class_1_prob'] >= 0.5).cache()\n",
    "        class_0 = pred_unlabeled.filter('class_0_prob >= 0.5').count()\n",
    "        class_1 = pred_unlabeled.filter('class_1_prob >= 0.5').count()\n",
    "\n",
    "        pred_unlabeled = pred_unlabeled.withColumn(Label, when(col(\"class_0_prob\").between(0.6, 1), 0).when(\n",
    "            col(\"class_1_prob\").between(0.6, 1), 1).otherwise(-1))\n",
    "        \n",
    "        # No new credible predicted data added\n",
    "        if 0 in (class_0, class_1):\n",
    "            break\n",
    "            \n",
    "        # Add the credible predicted data according to the ratio\n",
    "        if class_0 > class_1:\n",
    "            ratio = round(class_1 * ratioNP_P) / class_0\n",
    "            class_0 = round(class_1 * ratioNP_P)\n",
    "            print(class_0, class_1, ratioNP_P, ratio)\n",
    "            if ratio <= 1:\n",
    "                to_add_class0 = zero.sample(ratio)\n",
    "                to_add_class1 = one\n",
    "            else:\n",
    "                to_add_class0 = zero\n",
    "                to_add_class1 = one.sample(1/ratioNP_P)\n",
    "        else:\n",
    "            ratio = round(class_0 / ratioNP_P) / class_1\n",
    "            class_1 = round(class_0 / ratioNP_P)\n",
    "            if ratio <= 1:\n",
    "                to_add_class0 = zero\n",
    "                to_add_class1 = one.sample(ratio)\n",
    "            else:\n",
    "                to_add_class0 = zero\n",
    "                to_add_class1 = one.sample(1/ratioNP_P)\n",
    "\n",
    "\n",
    "        print(f'Adding {class_0} instances from class 0, and {class_1} from class 1')\n",
    "        print(f'Confidences time: {time.time() - confidences}')\n",
    "        \n",
    "        # fix the dataframe of the credible prediction data that will be added to the training set\n",
    "        # column should be same with the training data\n",
    "        to_add_class0 = to_add_class0.drop('rawPrediction', 'probability', 'xs', 'class_0_prob', 'class_1_prob',\n",
    "                                           'prediction').cache()\n",
    "        to_add_class1 = to_add_class1.drop('rawPrediction', 'probability', 'xs', 'class_0_prob', 'class_1_prob',\n",
    "                                           'prediction').cache()\n",
    "        unlabeled = unlabeled.subtract(to_add_class0).subtract(to_add_class1)\n",
    "        to_add_class0 = to_add_class0.withColumn(Label, lit(0))\n",
    "        to_add_class1 = to_add_class1.withColumn(Label, lit(1))\n",
    "\n",
    "        update = time.time()\n",
    "        \n",
    "        #process the labeled and unlabeled dataset\n",
    "        labeled = labeled.union(to_add_class0).union(to_add_class1)\n",
    "        \n",
    "        \n",
    "        pred_unlabeled.unpersist()\n",
    "\n",
    "        print(f'Updating sets time: {time.time() - update}')\n",
    "    \n",
    "    print(f'Iteration: {i} - labeled size: {labeled.count()},unlabeled size:{unlabeled.count()}')\n",
    "    training = time.time()\n",
    "    pipeline_model = pipeline.fit(labeled)\n",
    "    print(f'Training time: {time.time() - training}')\n",
    "    # test the model\n",
    "    final_pred_inductive = pipeline_model.transform(test)\n",
    "    final_pred_inductive = final_pred_inductive.withColumn('TP', when((final_pred_inductive['Cytoplasm']== 1) & (final_pred_inductive['prediction']==1), 1).otherwise(0))\n",
    "    final_pred_inductive = final_pred_inductive.withColumn('TF', when((final_pred_inductive['Cytoplasm']== 0) & (final_pred_inductive['prediction']==0), 1).otherwise(0))\n",
    "    final_pred_inductive = final_pred_inductive.withColumn('FP', when((final_pred_inductive['Cytoplasm']== 1) & (final_pred_inductive['prediction']==0), 1).otherwise(0))\n",
    "    final_pred_inductive = final_pred_inductive.withColumn('FN', when((final_pred_inductive['Cytoplasm']== 0) & (final_pred_inductive['prediction']==1), 1).otherwise(0))\n",
    "    \n",
    "    #compute the accuracy\n",
    "    TP = final_pred_inductive.filter('TP==1').count()\n",
    "    TN = final_pred_inductive.filter('TF==1').count()\n",
    "    FP = final_pred_inductive.filter('FP ==1').count()\n",
    "    FN = final_pred_inductive.filter('FN==1').count()\n",
    "    acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "    S = TP/(TP+TN)\n",
    "    E = TN/(TN+FP)\n",
    "    return acc,S,E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 - labeled size: 367,unlabeled size:14002\n",
      "Training time: 1.4723291397094727\n",
      "4377 3946 1.1091954022988506 0.43526252983293556\n",
      "Adding 4377 instances from class 0, and 3946 from class 1\n",
      "Confidences time: 1.0537374019622803\n",
      "Updating sets time: 0.04228544235229492\n",
      "Iteration: 1 - labeled size: 6635,unlabeled size:7690\n",
      "Training time: 1.9286463260650635\n",
      "4226 3810 1.1091954022988506 1.0891752577319587\n",
      "Adding 4226 instances from class 0, and 3810 from class 1\n",
      "Confidences time: 8.09591555595398\n",
      "Updating sets time: 0.06188797950744629\n",
      "Iteration: 2 - labeled size: 13069,unlabeled size:1256\n",
      "Training time: 87.8832221031189\n",
      "Adding 509 instances from class 0, and 459 from class 1\n",
      "Confidences time: 16.752129077911377\n",
      "Updating sets time: 0.10623836517333984\n",
      "Iteration: 2 - labeled size: 13856,unlabeled size:469\n",
      "Training time: 229.89652395248413\n"
     ]
    }
   ],
   "source": [
    "acc, S, E = train_the_classifer(df,3,\"Cytoplasm\",0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5789473684210527, 0.7662337662337663, 0.4090909090909091)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result\n",
    "acc,S,E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
