{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Co-training\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option(\"header\",'True').load(\"traincotraining.csv\")\n",
    "#read the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "for col in df.columns:\n",
    "     df = df.withColumn(col, df[col].astype(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_x = VectorAssembler(inputCols = [\"I\",\"V\",\"L\",\"F\",\"C\",\"M\",\"A\",\"G\",\"T\",\"S\",\"hdHydro_mean\"], outputCol = \"features_x\")\n",
    "va_y = VectorAssembler(inputCols = [\"W\",\"Y\",\"P\",\"H\",\"E\",\"Q\",\"D\",\"N\",\"K\",\"R\",\"helical_mean\"], outputCol = \"features_y\")\n",
    "df = va_x.transform(df)\n",
    "df = va_y.transform(df)\n",
    "# create the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labeled_x = df.filter('Cytoplasm_x != -1')\n",
    "labeled_y = df.filter('Cytoplasm_y != -1')\n",
    "unlabeled = df.subtract(labeled_x).subtract(labeled_y)\n",
    "# divide into 3 dataframe\n",
    "ratio = labeled_x.filter(\"Cytoplasm_x == 0\").count() / labeled_x.filter(\"Cytoplasm_x == 1\").count()\n",
    "# the initial ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXITER = 3\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 - labeled size x: 367,labeled size y:367,unlabeled size : 13944\n",
      "1.Training time: 2.359300374984741\n",
      "2.Confidences time: 11.482417345046997\n",
      "3.Updating sets time: 0.09477710723876953\n",
      "Iteration: 1 - labeled size x: 1429,labeled size y:1147,unlabeled size : 12165\n",
      "1.Training time: 22.318379878997803\n",
      "2.Confidences time: 81.86222696304321\n",
      "3.Updating sets time: 0.4574544429779053\n",
      "Iteration: 2 - labeled size x: 3668,labeled size y:5691,unlabeled size : 6354\n",
      "1.Training time: 80.32440090179443\n",
      "2.Confidences time: 199.50757265090942\n",
      "3.Updating sets time: 2.3138020038604736\n"
     ]
    }
   ],
   "source": [
    "for i in range(MAXITER):\n",
    "    \n",
    "    print(f'Iteration: {i} - labeled size x: {labeled_x.count()},labeled size y:{labeled_y.count()},unlabeled size : {unlabeled.count()}')\n",
    "    \n",
    "    training = time.time()\n",
    "    \n",
    "    # build model\n",
    "    rf_x = RandomForestClassifier(labelCol='Cytoplasm_x',featuresCol=\"features_x\",maxDepth=3,numTrees=16,seed=1234567)\n",
    "    rf_y = RandomForestClassifier(labelCol='Cytoplasm_y',featuresCol=\"features_y\",maxDepth=3,numTrees=16,seed=1234567)\n",
    "    pipe_x = Pipeline (stages = [rf_x])\n",
    "    pipe_y = Pipeline (stages = [rf_y])\n",
    "    # training\n",
    "    model_x = pipe_x.fit(labeled_x)\n",
    "    model_y = pipe_y.fit(labeled_y)\n",
    "    \n",
    "    print(f'1.Training time: {time.time()-training}')\n",
    "    # predict\n",
    "    pred_unlabeled_x = model_x.transform(unlabeled).cache()\n",
    "    pred_unlabeled_y = model_y.transform(unlabeled).cache()\n",
    "    \n",
    "    pred_unlabeled_x = pred_unlabeled_x.withColumn('xs', vector_to_array('probability'))\n",
    "    pred_unlabeled_x = pred_unlabeled_x.withColumn('class_0_prob_x', F.col('xs')[0])\n",
    "    pred_unlabeled_x = pred_unlabeled_x.withColumn('class_1_prob_x', F.col('xs')[1])\n",
    "    \n",
    "    pred_unlabeled_y = pred_unlabeled_y.withColumn('ys', vector_to_array('probability'))\n",
    "    pred_unlabeled_y = pred_unlabeled_y.withColumn('class_0_prob_y', F.col('ys')[0])\n",
    "    pred_unlabeled_y = pred_unlabeled_y.withColumn('class_1_prob_y', F.col('ys')[1])\n",
    "    \n",
    "    zero_x = pred_unlabeled_x.filter('class_0_prob_x >= 0.6')\n",
    "    one_x = pred_unlabeled_x.filter('class_1_prob_x >= 0.6')\n",
    "    zero_y = pred_unlabeled_y.filter('class_0_prob_y >= 0.6')\n",
    "    one_y = pred_unlabeled_y.filter('class_1_prob_y >= 0.6')\n",
    "    \n",
    "    class_0_x = zero_x.count()\n",
    "    class_1_x = one_x.count()\n",
    "    class_0_y = zero_y.count()\n",
    "    class_1_y = one_y.count()\n",
    "    # collect the credible data \n",
    "    if 0 in (class_0_x, class_1_x, class_0_y, class_1_y):\n",
    "        print(\"i = \", i, \"break \\n\")\n",
    "        break\n",
    "\n",
    "    if class_0_x > class_1_x:\n",
    "        ratio_x = round(class_1_x * ratio) / class_0_x\n",
    "        class_0_x = round(class_1_x * ratio)\n",
    "        to_add_class0_y = zero_x.sample(ratio_x)\n",
    "        to_add_class1_y = one_x\n",
    "    else:\n",
    "        ratio_x = round(class_0_x / ratio) / class_1_x\n",
    "        class_1_x = round(class_0_x / ratio) \n",
    "        to_add_class0_y = zero_x\n",
    "        to_add_class1_y = one_x.sample(ratio_x)\n",
    "        \n",
    "    if class_0_y > class_1_y:\n",
    "        ratio_y = round(class_1_y * ratio) / class_0_y\n",
    "        class_0_y = round(class_1_y * ratio)\n",
    "        to_add_class0_x = zero_y.sample(ratio_y)\n",
    "        to_add_class1_x = one_y\n",
    "    else:\n",
    "        ratio_y = round(class_0_x / ratio)/ class_1_y\n",
    "        class_1_y = round(class_0_y / ratio) \n",
    "        to_add_class0_x = zero_y\n",
    "        to_add_class1_x = one_y.sample(ratio_y)\n",
    "    \n",
    "    print(f'2.Confidences time: {time.time()-training}')\n",
    "    # fix the dataframe as the column should be same when union and substract\n",
    "    to_add_class0_y = to_add_class0_y.drop('rawPrediction','probability','xs','class_0_prob_x','class_1_prob_x','prediction')\n",
    "    to_add_class1_y = to_add_class1_y.drop('rawPrediction','probability','xs','class_0_prob_x','class_1_prob_x','prediction')\n",
    "    to_add_class0_x = to_add_class0_x.drop('rawPrediction','probability','ys','class_0_prob_y','class_1_prob_y','prediction')\n",
    "    to_add_class1_x = to_add_class1_x.drop('rawPrediction','probability','ys','class_0_prob_y','class_1_prob_y','prediction')\n",
    "\n",
    "    delete_xy = to_add_class0_x.union(to_add_class1_x).union(to_add_class0_y).union(to_add_class1_y).distinct()\n",
    "    unlabeled = unlabeled.subtract(delete_xy)\n",
    "    \n",
    "    to_add_class0_x = to_add_class0_x.withColumn(\"Cytoplasm_x\",to_add_class0_x.Cytoplasm_x*0)\n",
    "    to_add_class1_x = to_add_class1_x.withColumn(\"Cytoplasm_x\",to_add_class1_x.Cytoplasm_x*(-1))\n",
    "    to_add_class0_y = to_add_class0_y.withColumn(\"Cytoplasm_y\",to_add_class0_y.Cytoplasm_y*0)\n",
    "    to_add_class1_y = to_add_class1_y.withColumn(\"Cytoplasm_y\",to_add_class1_y.Cytoplasm_y*(-1))\n",
    "    \n",
    "    update = time.time()\n",
    "    labeled_x = labeled_x.union(to_add_class0_x).union(to_add_class1_x)\n",
    "    labeled_y = labeled_y.union(to_add_class0_y).union(to_add_class1_y)\n",
    "   \n",
    "    \n",
    "    pred_unlabeled_x.unpersist()\n",
    "    pred_unlabeled_y.unpersist()\n",
    "    end = time.time()\n",
    "    print(f'3.Updating sets time: {time.time()-update}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we need to build a classifier assembler"
   ]
  },
    {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "model_x = pipe_x.fit(labeled_x)\n",
    "model_y = pipe_y.fit(labeled_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
   "# read the test dataset\n",
    "test = spark.read.format('csv').option(\"header\", 'true').load(\"testselftraining.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the test\n",
    "numerical_colstest = test.columns\n",
    "test = test.select([F.col(c).cast(\"float\").alias(c) for c in numerical_colstest])\n",
    "test = test.withColumn('Cytoplasm',when(test.Cytoplasm.between(2,6),1).otherwise(0))",
    "test = va_x.transform(test)\n",
    "test = va_y.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test data\n",
    "final_pred = model_x.transform(test)\n",
    "final_pred = final_pred.withColumn('xs', vector_to_array('probability'))\n",
    "final_pred = final_pred.withColumn('class_0_prob_x', F.col('xs')[0])\n",
    "final_pred = final_pred.withColumn('class_1_prob_x', F.col('xs')[1])\n",
    "final_pred = final_pred.drop('probability').drop('rawPrediction').drop('xs').drop('prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = model_y.transform(final_pred)\n",
    "final_pred = final_pred.withColumn('ys', vector_to_array('probability'))\n",
    "final_pred = final_pred.withColumn('class_0_prob_y', F.col('ys')[0])\n",
    "final_pred = final_pred.withColumn('class_1_prob_y', F.col('ys')[1])\n",
    "final_pred = final_pred.drop('probability').drop('rawPrediction').drop('ys').drop('x').drop('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "for col in final_pred.columns:\n",
    "    final_pred = final_pred.withColumn(\"prediction\", final_pred.prediction.astype(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "final_pred = final_pred.withColumn('prediction', \n",
    "    F.when(((F.col('class_1_prob_x')>=F.col('class_0_prob_x')) &(F.col('class_1_prob_x')>=F.col('class_0_prob_y'))) | ((F.col('class_1_prob_y')>=F.col('class_0_prob_x')) &(F.col('class_1_prob_y')>=F.col('class_0_prob_y'))), 1)\n",
    "    .otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred= final_pred.withColumn('TP', \n",
    "    when((final_pred['Cytoplasm']== 1) & (final_pred['prediction']==1), 1)\n",
    "    .otherwise(0))\n",
    "final_pred = final_pred.withColumn('TN', \n",
    "    when((final_pred['Cytoplasm']== 0) & (final_pred['prediction']==0), 1)\n",
    "    .otherwise(0))\n",
    "final_pred = final_pred.withColumn('FP', \n",
    "    when((final_pred['Cytoplasm']== 1) & (final_pred['prediction']==0), 1)\n",
    "    .otherwise(0))\n",
    "final_pred = final_pred.withColumn('FN', \n",
    "    when((final_pred['Cytoplasm']== 0) & (final_pred['prediction']==1), 1)\n",
    "    .otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = final_pred.filter('TP==1').count()\n",
    "TN = final_pred.filter('TN==1').count()\n",
    "FP = final_pred.filter('FP ==1').count()\n",
    "FN = final_pred.filter('FN==1').count()\n",
    "acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "S = TP/(TP+TN)\n",
    "E = TN/(TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.45112781954887216, 0.48333333333333334, 0.3563218390804598)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc,S,E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
